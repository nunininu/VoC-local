import os
import re
import io
import json
import uuid
import torch
import boto3
import logging
import psycopg2
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
from kafka import KafkaConsumer
from konlpy.tag import Okt
from keybert import KeyBERT
from sklearn.metrics.pairwise import cosine_similarity
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from sentence_transformers import SentenceTransformer

from psycopg2.extensions import connection as PgConnection

load_dotenv()

logging.getLogger("transformers").setLevel(logging.ERROR)

okt = Okt()
kw_model = KeyBERT()
sentence_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
kw_model_extended = KeyBERT("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
sentiment_model_path = "/sentiment_model/checkpoint-150"
sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_path)
sentiment_tokenizer = AutoTokenizer.from_pretrained("beomi/KcELECTRA-base")
sentiment_model.eval()

KOREAN_STOPWORDS = [
    "ê·¸ëƒ¥", "ê·¸ë¦¬ê³ ", "ê·¼ë°", "ìŒ", "ì•„", "ì–´", "ë­", "ê·¸ì£ ", "ê·¸ëŸ¬ë‹ˆê¹Œ",
    "ê·¸ê±°", "ì €ê¸°", "ë¶€í„°", "ê·¸ê²Œ", "ì „í™”", "ì „í™”ê¸°", "ìˆ˜ê³ ", "ëŒ€í•´"
]

category_map = {
    0: "ìš”ê¸ˆ ì•ˆë‚´", 1: "ìš”ê¸ˆ ë‚©ë¶€", 2: "ìš”ê¸ˆì œ ë³€ê²½", 3: "ì„ íƒì•½ì • í• ì¸", 4: "ë‚©ë¶€ ë°©ë²• ë³€ê²½",
    5: "ë¶€ê°€ì„œë¹„ìŠ¤ ì•ˆë‚´", 6: "ì†Œì•¡ ê²°ì œ", 7: "íœ´ëŒ€í° ì •ì§€/ë¶„ì‹¤/íŒŒì†", 8: "ê¸°ê¸°ë³€ê²½", 9: "ë‚´ì—­ ì¡°íšŒ",
    10: "ëª…ì˜/ë²ˆí˜¸/ìœ ì‹¬ í•´ì§€", 11: "ê²°í•©í• ì¸", 12: "ê°€ìž… ì•ˆë‚´", 13: "í†µí™”í’ˆì§ˆ", 14: "ìž¥ë¹„ ì•ˆë‚´",
    15: "ì¸í„°ë„· ì†ë„/í’ˆì§ˆ", 16: "ìš”ê¸ˆ í• ì¸", 17: "ì¸í„°ë„· ìž¥ì• /ê³ ìž¥", 18: "ì •ë³´ë³€ê²½", 19: "ì²­êµ¬ì„œ",
    20: "ë³µì§€í• ì¸ ë“±ë¡", 21: "ë‚©ë¶€í™•ì¸ì„œ", 22: "êµ°ìž…ëŒ€ ì¼ì‹œì •ì§€", 23: "ê¸°ê¸°í• ë¶€ ì•½ì •", 24: "ë©¤ë²„ì‹­ í˜œíƒ",
    25: "ë³´í—˜ ì•ˆë‚´", 26: "í•´ì™¸ë¡œë°", 27: "ëª…ì˜/ë²ˆí˜¸/ìœ ì‹¬ ì¼ì‹œì •ì§€", 28: "ê°œí†µì  ì•ˆë‚´", 29: "ëª…ì˜/ë²ˆí˜¸/ìœ ì‹¬ ë³€ê²½",
    30: "ìƒí’ˆ ì•ˆë‚´", 31: "ì œíœ´ì¹´ë“œë¡œ í• ì¸", 32: "ë°°ì†¡ë¬¸ì˜", 33: "ìž¥ê¸°ê³ ê° í˜œíƒ", 34: "ê¸°íƒ€ í• ì¸ í˜œíƒ",
    35: "ë²ˆí˜¸ ë³€ê²½", 36: "ë°ì´í„° ì˜µì…˜ ì•ˆë‚´", 37: "ì„¸ê¸ˆê³„ì‚°ì„œ", 38: "ì¸í„°ë„· ê°€ìž… ë° ë³€ê²½", 39: "ìœ„ì•½ê¸ˆ ì•ˆë‚´",
    40: "í•´ì§€ ì•ˆë‚´", 41: "ë³´í—˜ í•´ì§€", 42: "ë³´í—˜ ì²­êµ¬", 43: "ìžë™ ì´ì²´", 44: "ëª…ì˜/ë²ˆí˜¸/ìœ ì‹¬ ì¼ì‹œì •ì§€",
    45: "í•œë„ ì•ˆë‚´", 46: "íŒŒì†ë³´í—˜ ì•ˆë‚´", 47: "ì•½ì • ì•ˆë‚´", 48: "ì¹´ë“œ ë°°ì†¡", 49: "ë³´í—˜ í˜œíƒ",
    50: "ë‚©ë¶€ í™•ì¸ì„œ", 51: "ì²­êµ¬ì§€ ë³€ê²½", 52: "ë²ˆí˜¸ ì´ë™", 53: "ì¸í„°ë„·ì „í™”ê°€ìž… ë° ë³€ê²½", 54: "í™˜ë¶ˆ ì•ˆë‚´",
    55: "ì´ì „ ì„¤ì¹˜", 56: "ìœ ì‹¬ ë¬¸ì œ", 57: "ë°˜í’ˆ ì•ˆë‚´", 58: "ë³´í—˜ ì‹ ì²­", 59: "ë²ˆí˜¸ì´ë™"
}

DOMAIN_KEYWORDS = [
    "ì•½ì •", "í• ì¸", "ë°ì´í„°", "ê¸°ê¸°", "ë³€ê²½", "í•´ì§€", "ìš”ê¸ˆì œ", "ë©¤ë²„ì‹­", "ìœ„ì•½ê¸ˆ",
    "LTE", "ë¬´ì œí•œ", "ì´ë™í†µì‹ ", "í†µì‹ ì‚¬", "ìž¬ì•½ì •", "ê³ ê°", "ê³„ì•½", "ì¸ì¦", "ê°€ìž…",
    "ë¬¸ìž", "ìŠ¤íŒ¸", "ë‹¨ë§ê¸°", "ì„¤ì •", "ë‚©ë¶€", "ìš”ê¸ˆ", "í• ë¶€ê¸ˆ", "ë³€ê²½", "ì •ì§€", "ì¼ì‹œì •ì§€",
    "ìœ ì‹¬", "ì„œë¥˜", "ë²ˆí˜¸", "ëª…ì˜", "ê°œí†µ", "ë¡œë°"
]

VERB_SUFFIXES = re.compile(r"(ë‹¤|ìš”|ë°)$")

consumer = KafkaConsumer(
    "voc-consulting-raw",
    bootstrap_servers="43.200.162.199:9092",
    auto_offset_reset="latest",
    enable_auto_commit=True,
    value_deserializer=lambda x: json.loads(x.decode("utf-8")) if x else None,
    session_timeout_ms=60000,
    heartbeat_interval_ms=15000,
    max_poll_interval_ms=1200000
)

def get_db_connection() -> PgConnection:
    return psycopg2.connect(
        host=os.getenv("host"),
        port=os.getenv("port"),
        user=os.getenv("user"),
        password=os.getenv("password"),
        database=os.getenv("database")
    )

def validate_keywords(keywords: list) -> list:
    return [kw if kw != "ê¸ˆì œ" else "ìš”ê¸ˆì œ" for kw in keywords]

def predict_sentiment(text: str) -> bool:
    inputs = sentiment_tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
    with torch.no_grad():
        outputs = sentiment_model(**inputs)
        return torch.argmax(outputs.logits, dim=1).item() == 1

def preprocess_korean(text: str) -> str:
    tokens = [word for word, tag in okt.pos(text, stem=True) if tag in ["Noun", "Adjective"]]
    filtered = [t for t in tokens if t not in KOREAN_STOPWORDS]
    final = [t for t in filtered if not VERB_SUFFIXES.search(t)]
    return " ".join(final)

def similarity_filter(keywords: list, category_id: int, category_map: dict, domain_keywords: list = None, threshold: float = 0.1, fallback_top_n: int = 3) -> list:
    category_name = category_map.get(category_id)
    if not category_name:
        return []
    if not keywords:
        return okt.morphs(category_name)[:fallback_top_n]
    cat_embedding = sentence_model.encode([category_name])
    keyword_embeddings = sentence_model.encode(keywords)
    sims = cosine_similarity(cat_embedding, keyword_embeddings)[0]
    keyword_with_sim = list(zip(keywords, sims))
    sorted_keywords = sorted(keyword_with_sim, key=lambda x: x[1], reverse=True)[:fallback_top_n]
    filtered = [kw for kw, _ in sorted_keywords]
    if not filtered:
        domain_embedding = sentence_model.encode(domain_keywords)
        domain_sims = cosine_similarity(cat_embedding, domain_embedding)[0]
        domain_with_sim = list(zip(domain_keywords, domain_sims))
        sorted_domain = sorted(domain_with_sim, key=lambda x: x[1], reverse=True)
        filtered = [kw for kw, _ in sorted_domain[:fallback_top_n]]
    return filtered

def extract_customer_content(text: str) -> str:
    return " ".join(re.findall(r"ê³ ê°:\s*(.*)", text))

def extract_keywords(text: str, category_id: int, top_n: int = 5, threshold: float = 0.3) -> list:
    content = extract_customer_content(text)
    filtered = preprocess_korean(content)
    raw_keywords = [kw[0] for kw in kw_model_extended.extract_keywords(filtered, top_n=top_n)]
    return similarity_filter(raw_keywords, category_id, category_map, DOMAIN_KEYWORDS, threshold, top_n)

def save_analysis_result_to_rds(result: dict) -> None:
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("""
            INSERT INTO analysis_result (analysis_result_id, consulting_id, client_id, keywords, is_negative, created_datetime)
            VALUES (%s, %s, %s, %s, %s, %s);
        """, (
            result['analysis_result_id'], result['consulting_id'], result['client_id'],
            result['keywords'], result['is_negative'], result['created_datetime']
        ))
        conn.commit()
        cur.close()
        conn.close()
        print("âœ… RDS ì €ìž¥ ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ RDS ì €ìž¥ ì‹¤íŒ¨: {e}")

def save_analysis_result_to_s3_parquet(result: dict, bucket_name: str) -> None:
    import traceback
    try:
        s3 = boto3.client("s3",
            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
            region_name=os.getenv("AWS_REGION")
        )
        dt = datetime.fromisoformat(result["created_datetime"]) if isinstance(result["created_datetime"], str) else result["created_datetime"]
        prefix = f"meta/analysis_result/year={dt.year}/month={dt.month:02d}/"
        key = f"{prefix}{result['analysis_result_id']}.parquet"
        buffer = io.BytesIO()
        pd.DataFrame([result]).to_parquet(buffer, index=False)
        buffer.seek(0)
        s3.put_object(Bucket=bucket_name, Key=key, Body=buffer.getvalue())
        print(f"âœ… S3 ì €ìž¥ ì™„ë£Œ: s3://{bucket_name}/{key}")
    except Exception as e:
        print(f"âŒ S3 ì €ìž¥ ì‹¤íŒ¨: {e}")
        traceback.print_exc()

def save_consulting_to_s3(data: dict, bucket_name: str, status: str = "SUCCESSED") -> None:
    try:
        dt = datetime.fromisoformat(data["consulting_datetime"])
        key = f"meta/consultings/year={dt.year}/month={dt.month:02d}/{data['consulting_id']}.parquet"
        buffer = io.BytesIO()
        pd.DataFrame([{**data, "analysis_status": status}]).to_parquet(buffer, index=False)
        buffer.seek(0)
        boto3.client("s3",
            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID"),
            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY"),
            region_name=os.getenv("AWS_REGION")
        ).put_object(Bucket=bucket_name, Key=key, Body=buffer.getvalue())
        print(f"âœ… ìƒë‹´ ë°ì´í„° ì €ìž¥ ì™„ë£Œ: {status}")
    except Exception as e:
        print(f"âŒ ìƒë‹´ S3 ì €ìž¥ ì‹¤íŒ¨: {e}")

def update_consulting_status_to_successed(consulting_id: str) -> None:
    try:
        conn = get_db_connection()
        cur = conn.cursor()
        cur.execute("""
            UPDATE consulting
            SET analysis_status = 'SUCCESSED'
            WHERE consulting_id = %s AND EXISTS (
                SELECT 1 FROM analysis_result WHERE consulting_id = %s
            )
        """, (consulting_id, consulting_id))
        conn.commit()
        cur.close()
        conn.close()
        print(f"âœ… ìƒë‹´ ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {consulting_id}")
    except Exception as e:
        print(f"âŒ ìƒë‹´ ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")

def consume_from_kafka() -> None:
    print("ðŸ“¡ Kafka ë©”ì‹œì§€ ìˆ˜ì‹  ëŒ€ê¸° ì¤‘...")
    for message in consumer:
        data = message.value
        print(f"ðŸ“© ìˆ˜ì‹  ë°ì´í„°: {data}")
        required = {"consulting_id", "content", "category_id", "client_id", "consulting_datetime"}
        if not required.issubset(data):
            print(f"âš  í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {data}")
            continue
        try:
            keywords = validate_keywords(extract_keywords(data["content"], data["category_id"]))
            is_negative = predict_sentiment(extract_customer_content(data["content"]))
            result = {
                "analysis_result_id": str(uuid.uuid4()),
                "consulting_id": data["consulting_id"],
                "client_id": data["client_id"],
                "keywords": ", ".join(keywords),
                "is_negative": is_negative,
                "created_datetime": datetime.now()
            }
            print(f"ðŸ“Š ë¶„ì„ ê²°ê³¼: {result}")
            save_analysis_result_to_rds(result)
            save_analysis_result_to_s3_parquet(result, os.getenv("AWS_S3_BUCKET_NAME"))
            save_consulting_to_s3(data, os.getenv("AWS_S3_BUCKET_NAME"), status="pending")
            update_consulting_status_to_successed(data["consulting_id"])
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            save_consulting_to_s3(data, os.getenv("AWS_S3_BUCKET_NAME"), status="pending")

def main() -> None:
    consume_from_kafka()

if __name__ == "__main__":
    main()

