import os
import re
import time
import random
from datetime import datetime, timedelta
import json
import pandas as pd
import logging
from kafka import KafkaConsumer
from google.cloud import storage
from konlpy.tag import Okt
from transformers import pipeline, XLMRobertaTokenizer
from keybert import KeyBERT
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path

# ë¡œê·¸ ì¤„ì´ê¸°
logging.getLogger("transformers").setLevel(logging.ERROR)

# ëª¨ë¸ ë¡œë”©
model_name = "joeddav/xlm-roberta-large-xnli"
tokenizer = XLMRobertaTokenizer.from_pretrained(model_name, use_fast=False)
classifier = pipeline("zero-shot-classification", model=model_name, tokenizer=tokenizer, device=0)
okt = Okt()
sentence_model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS", device="cuda")
kw_model_extended = KeyBERT(SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS", device="cuda"))

# ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸
KOREAN_STOPWORDS = [
    "ê·¸ëƒ¥", "ê·¸ë¦¬ê³ ", "ê·¼ë°", "ìŒ", "ì•„", "ì–´", "ë­", "ê·¸ì£ ", "ê·¸ëŸ¬ë‹ˆê¹Œ", "ê·¸ê±°", "ì €ê¸°", "ë¶€í„°", "ê·¸ê²Œ", "ì „í™”", "ì „í™”ê¸°", "ê³ ê°", "ìƒë‹´ì‚¬"
]

# ë„ë©”ì¸ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
DOMAIN_KEYWORDS = [
    "ì•½ì •", "í• ì¸", "ë°ì´í„°", "ê¸°ê¸°", "ë³€ê²½", "í•´ì§€", "ìš”ê¸ˆì œ", "ë©¤ë²„ì‹­", "ìœ„ì•½ê¸ˆ",
    "LTE", "ë¬´ì œí•œ", "ì´ë™í†µì‹ ", "í†µì‹ ì‚¬", "ì¬ì•½ì •", "ê³ ê°", "ê³„ì•½",
    "ì¸ì¦", "ê°€ì…", "ë¬¸ì", "ìŠ¤íŒ¸", "ë‹¨ë§ê¸°", "ì„¤ì •", "ë‚©ë¶€", "ìš”ê¸ˆ", "í• ë¶€ê¸ˆ", "ë³€ê²½",
    "ì •ì§€", "ì¼ì‹œì •ì§€", "ìœ ì‹¬", "ì„œë¥˜", "ë²ˆí˜¸", "ëª…ì˜", "ê°œí†µ", "ë¡œë°"
]
VERB_SUFFIXES = re.compile(r"(ë‹¤|ìš”|ë°)$")

# Kafka Consumer ì„¤ì •
consumer = KafkaConsumer(
    "voc-json",
    bootstrap_servers="localhost:9092",
    auto_offset_reset="earliest",
    enable_auto_commit=True,
    group_id="voc-group",
    value_deserializer=lambda x: json.loads(x.decode("utf-8")) if x else None,
    session_timeout_ms=60000,
    heartbeat_interval_ms=15000,
    max_poll_interval_ms=1200000
    )

# GCP ì„¤ì •
BUCKET_NAME = "wh04-voc"
PROCESSED_DATA_DIR = "meta/analysis_result/"

def save_parquet_to_gcs(bucket_name, data, consulting_date):
    client = storage.Client()
    bucket = client.get_bucket(bucket_name)

    try:
        dt = pd.to_datetime(consulting_date)
        hour = random.randint(0, 23)
        minute = random.randint(0, 11) * 5  # 5ë¶„ ë‹¨ìœ„

        # ìµœì¢… íŒŒí‹°ì…”ë‹ ê²½ë¡œ
        partition_path = f"year={dt.year}/month={dt.month:02}/day={dt.day:02}/hour={hour:02}/minute={minute:02}/"
        file_name = f"{dt.strftime('%Y%m%d')}_consulting_{data['consulting_id']}.parquet"
        blob_path = f"{PROCESSED_DATA_DIR}{partition_path}{file_name}"
        blob = bucket.blob(blob_path)

        if blob.exists():
            print(f"ğŸ›‘ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì…ë‹ˆë‹¤. ìŠ¤í‚µ: {blob_path}")
            return

        # Parquet íŒŒì¼ ì €ì¥
        df = pd.DataFrame([data])
        parquet_file = Path(f"~/temp/voc/{file_name}").expanduser()

        # ë¡œì»¬ íŒŒì¼ì´ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸
        if parquet_file.exists():
            print("ğŸ›‘ ì´ë¯¸ ë¡œì»¬ì— ì¡´ì¬í•˜ëŠ” íŒŒì¼ì…ë‹ˆë‹¤. ìŠ¤í‚µí•¨.")
        else:
            parquet_file.parent.mkdir(parents=True, exist_ok=True)  # ë””ë ‰í† ë¦¬ ìƒì„±
            df.to_parquet(parquet_file, index=False)
            blob.upload_from_filename(str(parquet_file))
            print(f"âœ… GCS ì—…ë¡œë“œ ì™„ë£Œ: {blob_path}")

    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: {e}")

def extract_customer_content(text):
    # ê³ ê° ë°œí™”ë§Œ ì¶”ì¶œ
    customer_lines = re.findall(r"ê³ ê°:\s*(.*)", text)
    return " ".join(customer_lines)

def preprocess_korean(text):
    # í˜•íƒœì†Œ ë¶„ì„ ë° ë¶ˆìš©ì–´&ì–´ë¯¸ ì œê±°
    tokens = [word for word, tag in okt.pos(text, stem=True) if tag == "Noun"]
    filtered_tokens = [token for token in tokens if token not in KOREAN_STOPWORDS]
    final_tokens = [token for token in filtered_tokens if not VERB_SUFFIXES.search(token)]
    return " ".join(final_tokens)

def similarity_filter(keywords, category, threshold=0.3, fallback_top_n=10):
    # ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ í•„í„°
    cat_embedding = sentence_model.encode([category])
    keyword_embeddings = sentence_model.encode(keywords)
    sims = cosine_similarity(cat_embedding, keyword_embeddings)[0]
    filtered = [kw for kw, sim in zip(keywords, sims) if sim >= threshold]
    return filtered if filtered else keywords[:fallback_top_n]

def extract_keywords(text, category, top_n=20, threshold=0.3):
    # 1. ê³ ê° ë°œí™”ë§Œ ì¶”ì¶œ
    customer_content = extract_customer_content(text)
    # 2. í˜•íƒœì†Œ ë¶„ë¦¬ ë° ë¶ˆìš©ì–´ ì œê±° (ë¬¸ë§¥ ìœ ì§€)
    filtered_text = preprocess_korean(customer_content)
    # 3. í‚¤ì›Œë“œ ì¶”ì¶œ (KeyBERT)
    raw_keywords = [kw[0] for kw in kw_model_extended.extract_keywords(filtered_text, top_n=top_n)]
    # 4. ì¹´í…Œê³ ë¦¬ ìœ ì‚¬ë„ í•„í„° ì ìš©
    final_keywords = similarity_filter(raw_keywords, category, threshold=threshold, fallback_top_n=top_n)
    return final_keywords

def process_message(data):
    try:
        consulting_id = data.get("consulting_id", "UNKNOWN")
        client_id = data.get("client_id", "UNKNOWN")
        consulting_content = data.get("consulting_content", "")
        consulting_category = data.get("consulting_category", "")
        consulting_date = data.get("consulting_date", "2020-01-01")
        duration = data.get("duration", None)

        text = extract_customer_content(consulting_content)
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = extract_keywords(text, consulting_category, 20, 0.3)

        # ê°ì • ë¶„ì„
        result = classifier(text, candidate_labels=["ë¶ˆë§Œ", "ê°ì‚¬"])
        negative_ratio = result["scores"][result["labels"].index("ë¶ˆë§Œ")]
        positive_ratio = result['scores'][result['labels'].index("ê°ì‚¬")]

        # ê²°ê³¼ í•©ì¹˜ê¸°
        analysis_result = {
            "consulting_id": consulting_id,
            "client_id": client_id,
            "keywords": ", ".join(keywords),
            "positive_ratio": positive_ratio,
            "negative_ratio": negative_ratio,
            "duration": duration if duration else None
        }

        # GCP ì €ì¥
        save_parquet_to_gcs(BUCKET_NAME, analysis_result, consulting_date)
        print(analysis_result)
    except Exception as e:
        print(f"âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

def main():
    for message in consumer:
        try:
            data = message.value
            if data is None:
                print("  ë¹ˆ ë©”ì‹œì§€, ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                continue
            process_message(data)
        except Exception as e:
            print(f"âŒ ë©”ì‹œì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()
